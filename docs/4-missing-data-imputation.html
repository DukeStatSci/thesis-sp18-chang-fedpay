<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Modeling Federal employees data to predict promotions</title>
  <meta name="description" content="Modeling Federal employees data to predict promotions">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Modeling Federal employees data to predict promotions" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Modeling Federal employees data to predict promotions" />
  
  
  

<meta name="author" content="Chia-Rui (Jerry) Chang">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="3-this-chunk-ensures-that-the-thesisdowndss-package-is.html">
<link rel="next" href="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> If you are creating a PDF you’ll need to write your preliminary content here or</a></li>
<li class="chapter" data-level="" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="2" data-path="2-introduction.html"><a href="2-introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="3-this-chunk-ensures-that-the-thesisdowndss-package-is.html"><a href="3-this-chunk-ensures-that-the-thesisdowndss-package-is.html"><i class="fa fa-check"></i><b>3</b> This chunk ensures that the thesisdowndss package is</a></li>
<li class="chapter" data-level="4" data-path="4-missing-data-imputation.html"><a href="4-missing-data-imputation.html"><i class="fa fa-check"></i><b>4</b> Missing Data Imputation</a><ul>
<li class="chapter" data-level="4.1" data-path="4-missing-data-imputation.html"><a href="4-missing-data-imputation.html#missing-data"><i class="fa fa-check"></i><b>4.1</b> Missing Data</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-missing-data-imputation.html"><a href="4-missing-data-imputation.html#addressing-missing-data"><i class="fa fa-check"></i><b>4.1.1</b> Addressing Missing Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-missing-data-imputation.html"><a href="4-missing-data-imputation.html#overview-of-multiple-imputation"><i class="fa fa-check"></i><b>4.2</b> Overview of Multiple Imputation</a></li>
<li class="chapter" data-level="4.3" data-path="4-missing-data-imputation.html"><a href="4-missing-data-imputation.html#specifying-predictive-conditional-distribution---the-cart-method"><i class="fa fa-check"></i><b>4.3</b> Specifying predictive conditional distribution - the CART method</a></li>
<li class="chapter" data-level="4.4" data-path="4-missing-data-imputation.html"><a href="4-missing-data-imputation.html#improving-computational-efficiency"><i class="fa fa-check"></i><b>4.4</b> Improving computational efficiency</a></li>
<li class="chapter" data-level="4.5" data-path="4-missing-data-imputation.html"><a href="4-missing-data-imputation.html#imputation-results-and-predictive-checks"><i class="fa fa-check"></i><b>4.5</b> Imputation results and predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html"><a href="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html"><i class="fa fa-check"></i><b>5</b> This chunk ensures that the thesisdowndss package is</a><ul>
<li class="chapter" data-level="5.1" data-path="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html"><a href="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html#outcome-variable-and-covariates-selection"><i class="fa fa-check"></i><b>5.1</b> Outcome variable and covariates selection</a></li>
<li class="chapter" data-level="5.2" data-path="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html"><a href="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html#inspection-of-each-covariate"><i class="fa fa-check"></i><b>5.2</b> Inspection of each covariate</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html"><a href="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html"><i class="fa fa-check"></i><b>6</b> This chunk ensures that the thesisdowndss package is</a><ul>
<li class="chapter" data-level="6.1" data-path="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html"><a href="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html#logistic-regression-with-piecewise-linear-spline-application"><i class="fa fa-check"></i><b>6.1</b> Logistic regression with piecewise linear spline application</a></li>
<li class="chapter" data-level="6.2" data-path="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html"><a href="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html#baseline"><i class="fa fa-check"></i><b>6.2</b> Baseline</a></li>
<li class="chapter" data-level="6.3" data-path="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html"><a href="6-this-chunk-ensures-that-the-thesisdowndss-package-is-2.html#model-form"><i class="fa fa-check"></i><b>6.3</b> Model Form</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><i class="fa fa-check"></i><b>7</b> This chunk ensures that the thesisdowndss package is</a><ul>
<li class="chapter" data-level="7.1" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html#estimates-of-model-coefficients"><i class="fa fa-check"></i><b>7.1</b> Estimates of model coefficients</a></li>
<li class="chapter" data-level="7.2" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html#pooled-results"><i class="fa fa-check"></i><b>7.2</b> Pooled results</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html#frequentist-model"><i class="fa fa-check"></i><b>7.2.1</b> Frequentist model</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html#bayesian-model"><i class="fa fa-check"></i><b>7.2.2</b> Bayesian Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html#comparison-between-frequentist-and-bayesian-model"><i class="fa fa-check"></i><b>7.2.3</b> Comparison between Frequentist and Bayesian Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html#inference-based-on-frequentist-model"><i class="fa fa-check"></i><b>7.3</b> Inference based on Frequentist model</a></li>
<li class="chapter" data-level="7.4" data-path="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html"><a href="7-this-chunk-ensures-that-the-thesisdowndss-package-is-3.html#conclusion-and-future-work"><i class="fa fa-check"></i><b>7.4</b> Conclusion and future work</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Federal employees data to predict promotions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="missing-data-imputation" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Missing Data Imputation</h1>
<div id="missing-data" class="section level2">
<h2><span class="header-section-number">4.1</span> Missing Data</h2>
<p>[In social science research, survey item-nonresponse and data recording errors often lead to missing data. The OPM data is no exception.] Rubin classified missing data into three properties: missing completely at random (MCAR), missing at random (MAR), and not missing at random (NMAR) <span class="citation">(Little &amp; Rubin, 2014)</span>. First, data has MCAR property when the probability of observations being missing is unrelated to any variables. Second, data has MAR property when the probability of missing only depends on observed values but not on unobserved values. Though there are no formal ways to verify the MAR property, it is usually a reasonable assumption when we do not have comprehensive knowledge of the data. Third, data that cannot be attributed to either MCAR or MAR has NMAR property - the probability of missing depends on both observed and unobserved values. To assess missing mechanisms of the OPM data, we examined the nature and patterns of variables with missing data. In general, the missingness of the OPM data can be summarized into two categories - inherent missingness and missingness due to time constraint.</p>
<p>First, the OPM data contain more than 20 years of federal employees’ information. As rules and standards change overtime, recording errors would naturally generate missing data. For example, <em>Race</em> variable was originally classified into 16 categories, but new standard established in 2016 simplified it into 6 categories. The main strategy to address the missingness of <em>Race</em> variable is to convert old recording standard to new standard. However, small percentage (less than 0.01%) of data is still missing even after conversion. <em>Education Level</em> is another variable that has missing data. The fact that employees might not fill in and update their personal information could lead to the missingness of <em>Education Level</em> (Figure <a href="4-missing-data-imputation.html#fig:fixedmissing">4.1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:fixedmissing"></span>
<img src="figure/fixedmissing.png" alt="Inherent Missingness Independent of Time" width="400px" />
<p class="caption">
Figure 4.1: Inherent Missingness Independent of Time
</p>
</div>
<p>Second, the OPM database was not created until 1988, so information regarding employees started before 1988 would not be recorded. For example, if an employee worked between 1980 and 1990, the OPM data would only record this employee’s information from 1988 to 1990, but data from 1980 to 1987 would be missing. Variables that change overtime belong to this category. As mentioned previously, a Federal employee’s salary is determined by particular grade, step rate and pay plan. Since <em>Pay plan</em>, <em>Grade</em>, <em>Step Rate</em>, and <em>Salary</em> are associated, these four variables have the same missing patterns. Specifically, the proportion of missing data decreases as year increases (Figure <a href="4-missing-data-imputation.html#fig:nonfixedmissing">4.2</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:nonfixedmissing"></span>
<img src="figure/nonfixedmissing.png" alt="Missingness due to Time Constraint" width="400px" />
<p class="caption">
Figure 4.2: Missingness due to Time Constraint
</p>
</div>
<p>Regardless of time constraint, the missingness of variables is related to the characteristics of observed variables. For example, employees with higher education level are more likely to receive higher starting salary. In other words, the missing salary information is related to observed education level and other variables, confirming that the data are not MCAR. Because no additional information indicates the NMAR property, we should be fairly comfortable to assume MAR for the OPM data. </p>
<div id="addressing-missing-data" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Addressing Missing Data</h3>
<p>The loss of information from missing data would produce bias and impact the robustness of statistical modeling and inference. To address this problem, it is necessary to impute the missing data. In the next section, we would explore various imputation methods and strategies depending on the structure and the nature of the missing data.</p>
</div>
</div>
<div id="overview-of-multiple-imputation" class="section level2">
<h2><span class="header-section-number">4.2</span> Overview of Multiple Imputation</h2>
<p>Multiple Imputation (MI) is a flexible approach for researchers to address the problem of missing data. The method for carrying out multiple imputation involves: (1) fill in the missing values through drawing values from predictive conditional distribution to produce <span class="math inline">\(m\)</span> completed datasets; (2) for each completed dataset, conduct analysis for parameters of interest; and (3) combine individual analysis to form final results <span class="citation">(Little &amp; Rubin, 2014)</span>. Multivariate imputation by chained equation (MICE), where multiple imputed datasets are created through drawing samples sequentially from predictive conditional distribution, is often applied to generate multiple completed datasets at step (1). MICE is one of the most popular approach for imputing missing data. It provides flexible imputation results and can handle both continuous and categorical data. The implementation of MICE involves the following steps.</p>
<p>Here, we assume we have a <span class="math inline">\(n \times k\)</span> data matrix <span class="math inline">\(Y\)</span>, where <span class="math inline">\(Y_1, Y_2, .., Y_j\)</span> are completely observed and <span class="math inline">\(Y_{j+1}, Y_{j+2}, ..., Y_k\)</span> are partially observed.</p>
<p><strong>Step 1</strong><br />
For <span class="math inline">\(i = j+1, ... , k\)</span>, fill in initial missing values of <span class="math inline">\(Y_i\)</span> through draws from predictive conditional distribution <span class="math inline">\(Y_i|Y_1, Y_2, ..., Y_j\)</span>.<br />
<strong>Step 2</strong><br />
For <span class="math inline">\(i = j + 1,...,k\)</span>,<br />
(a) construct a predictive distribution for <span class="math inline">\(Y_{i}\)</span> fully conditional on <span class="math inline">\(Y_{-i} = Y_1, Y_2, ... , Y_{i-1}, Y_{i+1}, ... , Y_k\)</span>.<br />
(b) Draw values from the conditional distribution <span class="math inline">\(Y_i|Y_{-i}\)</span> and update the original missing values of column <span class="math inline">\(Y_i\)</span>.<br />
<strong>Step 3</strong> Perform step 2 for <span class="math inline">\(l\)</span> times.<br />
<strong>Step 4</strong> Perform step 1-3 for <span class="math inline">\(m\)</span> times to generate <span class="math inline">\(m\)</span> completed datasets. </p>
<p>We applied MICE to impute missing values in the OPM data, where <span class="math inline">\(l = 20\)</span> and <span class="math inline">\(m = 12\)</span>. Since the robustness of MICE is dependent on the predictive conditional distribution, one strategy to improve imputation result is to include any “auxiliary variables” that are related to the missingness but not part of the covariates. The inclusion of “auxiliary variables” can make the MAR assumption more reasonable <span class="citation">(Collins, Schafer, &amp; Kam, 2001)</span>. In the OPM data, variables such as <em>Pay Plan</em> and <em>Step Rate</em> are not parameters of interest to the final analysis. However, both variables provide information on the missingness of <em>Salary</em> and <em>Grade</em>, so we included them in the imputation model. Another auxiliary variable we included is salary between working year 11 and 15. Though we are only interested in the rate of change in salary between working year 1 and 10, salary between working year 11 and 15 can help predict past salary change rate. </p>
</div>
<div id="specifying-predictive-conditional-distribution---the-cart-method" class="section level2">
<h2><span class="header-section-number">4.3</span> Specifying predictive conditional distribution - the CART method</h2>
<p>In the practice of MICE, one of the most common models for specifying predictive conditional distribution is Generalized Linear Models (GLMs). GLMs such as multiple linear regressions often produce consistent imputation results. However, if the data to be imputed contain hundreds of variables, GLMs might be too simple to capture the true distribution. For example, relationships among variables might be interactive and non-linear. Specifying parametric models for data with great complexity is therefore inappropriate <span class="citation">(Burgette &amp; Reiter, 2010)</span>. The OPM data contains 65 variables. Eeach categorical variable has various levels. Since non-linear relationships might exist among multiple variables and levels, specifying the standard GLMs on the conditional distribution could lead to biased parameter estimates and produce inconsistent results. To address this challenge, non-parametric model can be more appropriate; specifically, we used Classification and Regression Trees (CART) to impute missing data. </p>
<p>The CART algorithm performs binary splits of the predictors recursively to approximate the conditional distribution of an univariate outcome. The partitions are found if the subsets of units have relatively homogeneous outcomes. The leaf would be reached after multiple partitions, with values in each leaf representing the conditional distribution of the outcome. If the outcome variable is categorical, Classification Tree would be adopted; on the other hand, Regression Tree would be implemented if the outcome variable is continuous. For its application in MICE, we would use CART to derive the conditional distribution for each <span class="math inline">\(Y_i\)</span> on the completely observed variables in step 1 and each <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(Y_{-i}\)</span> in step 2 <span class="citation">(Burgette &amp; Reiter, 2010)</span> <span class="citation">(Doove, Van Buuren, &amp; Dusseldorp, 2014)</span>. </p>
<p>Though one of the disadvantages of the CART method is its difficulty for model interpretations when the number of tree level is high, it should not be our major concern because the goal is to plausibly impute the missing data. Indeed, the application of non-parametric CART models in MICE can result in “more reliable inferences compared with naive applications of MICE” <span class="citation">(Burgette &amp; Reiter, 2010)</span>.</p>
</div>
<div id="improving-computational-efficiency" class="section level2">
<h2><span class="header-section-number">4.4</span> Improving computational efficiency</h2>
<p>The CART method is computationally efficient if predictors to be split are continuous variables. On the other hand, if predictors to be split are categorical variables, the CART method might encounter computational difficulties when the variable has multiple levels. If a categorical variable has <span class="math inline">\(n\)</span> levels, the CART method would examine every possible splits, which results to <span class="math inline">\(2^{n}\)</span> possible partitions. In the OPM data, variable <em>Grade</em> has more than 20 levels, which means there are more than <span class="math inline">\(2^{20} \approx 1\)</span> million possible partitions.</p>
<p>One solution to increase the computational efficiency of the CART imputation method is to reduce the number of levels for categorical variables. For example, we reduced <em>Pay Plan</em> from 173 categories into 7 categories. The OPM defines pay plan as “a two-digit alphabetical code used to identify Federal civilian pay systems” <span class="citation">(Office of Personal Management (OPM), n.d.)</span>. The most common pay plan in the OPM data is the General Schedule pay system, which covers around 78 percent of white-collar Federal employees. Other pay plans cover employees who have unique occupations or serve for particular agency. For example, the <em>AL</em> pay plan applies to administrative law judges, and <em>SV</em> refers to pay plan in the Transportation Security Administration. Because some pay plan codes are only applicable to relatively small subset of population, we decide to merge pay plans with small sample size into “others” category. The table below is the simplified pay plan after merging.</p>
<table>
<caption><em>Simplified Pay Plan</em></caption>
<thead>
<tr class="header">
<th align="left"><strong>Type</strong></th>
<th align="left"><strong>Codes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">General Schedule</td>
<td align="left">GS, GM</td>
</tr>
<tr class="even">
<td align="left">Non General Schedule</td>
<td align="left">AD, ES, SV, VN, Others</td>
</tr>
</tbody>
</table>
<p>Another strategy used to increase the computational efficiency is through applying parallel computing technique on <em>Step 4</em> of the MICE. The implementation of the parallel computing includes the following steps.</p>
<p>Here, we assume we want to impute <span class="math inline">\(m\)</span> completed datasets with <span class="math inline">\(n\)</span> cores.</p>
<p><strong>If (<span class="math inline">\(m \leq n\)</span>)</strong><br />
allocate <span class="math inline">\(m\)</span> cores<br />
conduct <em>Step 1</em> to <em>Step 3</em> of MICE once for each core in parallel</p>
<p><img src="figure/parallel_less.png" scale=0.5 style="display: block; margin: auto;" /></p>
<p><strong>else</strong><br />
allocate <span class="math inline">\(n\)</span> cores<br />
conduct <em>Step 1</em> to <em>Step 3</em> of MICE <span class="math inline">\(\lfloor{\frac{n}{m}}\rfloor\)</span> times for each core in parallel</p>
<p><img src="figure/parallel_more.png" scale=0.5 style="display: block; margin: auto;" /></p>
</div>
<div id="imputation-results-and-predictive-checks" class="section level2">
<h2><span class="header-section-number">4.5</span> Imputation results and predictive checks</h2>
<p>We generated 12 completed datasets with the CART imputation model. The validity of the imputed datasets depends on the use of an appropriate imputation model, so it is important to check whether the model yields reasonable results. We applied predictive checks to assess imputation model adequacy. Predictive checks is a Bayesian model checking technique designed to investigate the potential model inadequacy between imputed and replicated datasets. Though our imputation model is not fully Bayesian, predictive checks could still be used to measure the predictive differences between imputed and replicated data <span class="citation">(He &amp; Zaslavsky, 2012)</span>.</p>
<p>Denote <span class="math inline">\(Y = (Y_{1}, Y_{2}) = ((Y_{1,obs}, Y_{1,mis}), Y_{2}))\)</span>, where <span class="math inline">\(Y_{1}\)</span> is partially observed and <span class="math inline">\(Y_{2}\)</span> is completely observed. To generate replicated data <span class="math inline">\(Y_{com}^{rep}\)</span>, the steps involve: (1) create duplicates of <span class="math inline">\((Y_{1}, Y_{2})\)</span>; (2) set duplicated <span class="math inline">\(Y_{1}\)</span> as completely missing; (3) combine <span class="math inline">\(((Y_{1,obs}, Y_{1,mis}), Y_{2})\)</span> and <span class="math inline">\((Y_{com}^{rep}, Y_{2})\)</span> to form concatenated dataset; and (4) re-impute concatenated dataset with original imputation model. Figure <a href="4-missing-data-imputation.html#fig:PPC">4.3</a> <span class="citation">(He &amp; Zaslavsky, 2012)</span> shows the re-imputation process.</p>
<div class="figure" style="text-align: center"><span id="fig:PPC"></span>
<img src="figure/PPC.jpg" alt="Generate replicated dataset"  />
<p class="caption">
Figure 4.3: Generate replicated dataset
</p>
</div>
<p>Our parameter of interest is the relationship between year 1 and year 10 salary. We assessed model adequecy through comparing the distribution of working year 1 and year 10 salary between imputed and replicated datasets. If the distribution of imputed datasets and the distribution of replicated datasets are similar, then imputation model provides plausible fit to the data. We generated replicated dataset with two methods. First, we conducted re-imputation 12 times by setting duplicated <span class="math inline">\(Y_{1}\)</span> (variables with missing values) as completely missing. Second, we conducted re-imputation 12 times by only setting salary at working year 1 and 10 in duplicated <span class="math inline">\(Y_{1}\)</span> as completely missing. Figure <a href="4-missing-data-imputation.html#fig:PPC1">4.4</a> shows the result of the first method, and Figure <a href="4-missing-data-imputation.html#fig:PPC2">4.5</a> shows the result of the second method.</p>
<div class="figure" style="text-align: center"><span id="fig:PPC1"></span>
<img src="figure/PPC1.png" alt="Salary distribution of method 1"  />
<p class="caption">
Figure 4.4: Salary distribution of method 1
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:PPC2"></span>
<img src="figure/PPC2.png" alt="Salary distribution of method 2"  />
<p class="caption">
Figure 4.5: Salary distribution of method 2
</p>
</div>
<p>For both figures, the x-axis represents salary at working year 1, and the y-axis represents salary at working year 10. Since salaries for employees with SES and non-SES position tend to vary a lot, we visualized the salary distribution separately for employees with SES position and non-SES position. Note that both figures are visualization of one replicated and one imputed datasets. (We compared all 12 replicated and 12 imputed datasets. Since they all yielded similiar results, we only included one here.)</p>
<p>Figure <a href="4-missing-data-imputation.html#fig:PPC1">4.4</a> shows that the distributions between imputed and replicated datasets for both positions are different, indicating that our imputation model is not a good fit to the data. However, this method might not be ideal because the imputed values of year 1 and year 10 salary are dependent on other partially missing variables. The second method provides a better model diagnostic. Figure <a href="4-missing-data-imputation.html#fig:PPC2">4.5</a> shows that the distributions between imputed and replicated datasets for both positions are similar, implying that our imputation model provides plausible fit to the data.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-this-chunk-ensures-that-the-thesisdowndss-package-is.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-this-chunk-ensures-that-the-thesisdowndss-package-is-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
