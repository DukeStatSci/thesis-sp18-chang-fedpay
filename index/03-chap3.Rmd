---
output:
  pdf_document: default
  html_document: default
---

```{r, include = FALSE}
# This chunk ensures that the thesisdowndss package is
# installed and loaded. This thesisdowndss package includes
# the template files for the thesis and also two functions
# used for labeling and referencin
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("bookdown", repos = "http://cran.rstudio.com")
if(!require(thesisdowndss)){
  library(devtools)
  devtools::install_github("mine-cetinkaya-rundel/thesisdowndss")
  }
library(thesisdowndss)
```

# Imputation of Missing Data

##Overview of Multiple Imputation

Multiple Imputation (MI) is a flexible approach for researchers to address the problem of missing data. The approach of Multivariate Imputation by Chained Equation (MICE), where multiple imputed datasets are created through drawing samples sequentially from predictive conditional distribution, would be applied to generate multiple completed datasets.  
\hfill\break

The methods for carrying out multiple imputation involve (1) Fill in the missing columns through drawing values from predictive conditional distribution to produce $m$ completed datasets (2) For each completed dataset, conduct analysis for parameters of interest (3) Combine individual analysis to form final results. [@little2014statistical] Currently, different methods exist for specifying predictive conditional distributions at step (1). We would explore the suitability of each method and select the best approach.

**Multivariate Imputation by Chained Equations (MICE)**

To generate $m$ complete datasets, we would apply Multivariate Imputation by Chained Equations (MICE). MICE is one of the most popular approach for imputing missing data. It provides flexible imputation results and can handle both continuous and categorical data. The implementations of MICE involve

\hfill\break
Assume we have an $n$x$k$ data matrix $Y$, where $Y_1, Y_2, .., Y_j$ are completely observed and $Y_{j+1}, Y_{j+2}, ..., Y_k$ are partially observed.  

**Step 1**  
For $i = j+1, ... , k,$, fill in initial missing values of $Y_i$ through draws from predictive distribution conditional on $Y_1, Y_2, ..., Y_j$.  
**Step 2**  
  (a) For $i = j+1, ... , k,$, construct a predictive distribution fully conditional on $Y_{-i} = Y_1, Y_2, ... , Y_{i-1}, Y_{i+1}, ... , Y_k$.  
  (b) Draw values from the conditional distribution $Y_i|Y_{-i}$ and update the original missing values of column $Y_i$.  
**Step 3**
Perform step 2 $l$ times.  
**Step 4**
Perform step 1-3 $m$ times. 
\hfill\break

Since the robustness of MICE is highly dependent on the predictive conditional distribution, one of the strategy to improve the imputations is to include "auxiliary variable" that are related to the missingness but not part of the covariates. Indeed, the inclusion of "auxiliary variable" can make the MAR assumption more reasonable.  [@collins2001comparison] In the OPM data, variables such as _Pay plan_ and _Step Rate_ are not of interest to the final analysis, but they provide information on the missingness of _Salary_ and _Grade_. Another auxiliary variable we include is salary from working year 11 to 15. Though we are only interested in the rate of change in salary between working year 1 and 10, salary from working year 11 to 15 can help predict past salary information.
\hfill\break

##Specifying Predictive Conditional Distribution - the CART method

In the practice of MICE, one of the most common models for specifying predictive conditional distribution is Generalized Linear Models (GLMs). GLMs such as multiple linear regressions are flexible parametric models, and usually produce consistent imputation results. However, if the data to be imputed contain hundreds of variables, GLMs might be too simple to capture the true distribution. For example, relationships among variables might be interactive and non-linear. Specifying parametric models for data with great complexity is therefore inappropriate. [@burgette2010multiple]
\hfill\break

Currently, the OPM data contains 65 variables, and each categorical variable has various levels. Since non-linear relationships might exist among multiple variables and levels, specifying the standard GLMs on the conditional distribution would lead to biased parameter estimates and produce inconsistent results. To address this challenge, non-parametric model is more appropriate; specifically, Classification and Regression Trees (CART) would be used to impute missing data.
\hfill\break

The CART algorithm performs binary splits of the predictors recursively to approximate the conditional distribution of a univariate outcome. The partitions are found if the subsets of units have relatively homogeneous outcomes. The leaf would be reached after multiple partitions, with values in each leaf representing the conditional distribution of the outcome. If the outcome variable is categorical, Classification Tree would be adopted; on the other hand, Regression Tree would be implemented if the outcome variable is continuous. For its application in MICE, we would use CART to derive the conditional distribution for each $Y_i$ on the completely observed variables in step 1 and each $Y_i$ given $Y_{-i}$ in step 2. [@burgette2010multiple] 
[@doove2014recursive]
\hfill\break

Though one of the disadvantages of the CART method is its difficulty for model interpretations when the number of tree level is high, it should not be our major concern because the goal for adopting CART method is to plausibly impute the missing data. Indeed, the application of non-parametric CART models in MICE can result in "more reliable inferences compared with naive applications of MICE". [@burgette2010multiple]

##Improving Computational Efficiency

The CART method is computationally efficient if predictors to be split are continuous variables. Specifically, continuous data will be sorted in ascending order before partitions, which reduces the computational complexity. On the other hand, if predictors to be split are categorical variables, the CART method might encounter computational difficulties when the variable has multiple levels. If a categorical variable has $n$ levels, the CART method would examine every possible splits, which results to $2^{n}$ possible partitions. Indeed, variable _Grade_ has more than 20 levels, which means there are more than $2^{20} \approx 1$ million possible partitions.

One solution to increase the computational efficiency of the CART imputation method is to reduce the number of levels for categorical variables. For example, _Pay Plan_ is reduced from 173 categories into 7 categories. The OPM defines pay plan as "a two-digit alphabetical code used to identify Federal civilian pay systems". [@payplan2017] The most common pay plan in the OPM data is the General Schedule pay system, which covers around 78 percent of white-collar Federal employees. Employees who have unique occupations or serve for particular agency are covered by other pay plans. For example, the _AL_ pay plan applies to administrative law judges, and _SV_ refers to pay plan in the Transportation Security Administration. Since some of the pay plan codes are applicable to relatively small subset of population, we decide to merge pay plan from 173 categories into 7 categories. The summary of the simplified pay plan is shown in the table below.


**Type** | **Codes**
------------- | -----------------
General Schedule | GS, GM
Non General Schedule | AD, ES, SV, VN, Others    
Table: *Abbreviated Pay Plan*  


Another strategy used to increase the computational efficiency for MICE is through applying parallel computing technique on *Step 4* of the MICE. The implementation of the parallel computing includes: 

Assume we want to impute $m$ completed datasets with $n$ cores.

**If ($m \leq n$)**  
allocate $m$ cores  
conduct *Step 1* to *Step 3* once for each core in parallel  

```{r,out.extra="scale=0.5",echo=FALSE}
include_graphics(path = "figure/parallel_less.png")
```

**else**  
allocate $n$ cores  
conduct *Step 1* to *Step 3* $\lfloor{\frac{n}{m}}\rfloor$ times  for each core in parallel  

```{r,out.extra="scale=0.5",echo=FALSE}
include_graphics(path = "figure/parallel_more.png")
```

##Imputation Results and Posterior Predictive Checks

12 completed datasets are generated from the imputation model. The validity of the imputed datasets depends on the use of an appropriate imputation model, so it is important to check whether the model yields reasonable results. Specifically, posterior predictive checks (PPC) would be applied to assess imputation model adequacy. PPC is a Bayesian model checking technique designed to investigate the potential model inadequacy between imputed and replicated datasets. Though our imputation model is not fully Bayesian, PPC could still be applied to measure the predictive differences between imputed and replicated data. [@PPC]

Denote $Y = (Y_{1}, Y_{2}) = ((Y_{1,obs}, Y_{1,mis}), Y_{2}))$, where $Y_{1}$ is partially observed and $Y_{2}$ is completely observed. To generate replicated data $Y_{com}^{rep}$, we need to (1) create duplicates of $(Y_{1}, Y_{2})$ (2) set duplicated $Y_{1}$ as completely missing (3) combine $((Y_{1,obs}, Y_{1,mis}), Y_{2})$ and $(Y_{com}^{rep}, Y_{2})$ to form concatenated dataset (4) re-impute concatenated dataset with original imputation model. Figure \@ref(fig:PPC) shows the re-imputation process. [@PPC] Since our parameter of interest is the relationship between year 1 and year 10 salary, we compare the distribution of these variables between imputed and replicated datasets. If the distribution of these two variables are similar between imputed and replicated datasets, then we can conclude that the imputation model provides good fit to the data.

```{r PPC, fig.cap="Generate replicated dataset",echo=FALSE}
include_graphics(path = "figure/PPC.jpg")
```

Two different methods are used to generate replicated dataset. First, conduct re-imputation by setting duplicated $Y_{1}$ (variables with missing values) as completely missing. Second, conduct re-imputation by only setting salary at working year 1 and 10 in duplicated $Y_{1}$ as completely missing. The distribution between imputed and replicated dataset is shown by Figure \@ref(fig:PPC1) for the first method, and Figure \@ref(fig:PPC2) for the second method. The first method indicates that our imputation model is not a good fit to the data. However, this method might not be ideal because the imputed values of year 1 and year 10 salary are highly dependent on other partially missing variables. The second method provides a better model diagnostic. The distribution between imputed and replicated datasets are similar, which implies that our imputation model provides good fit to the data.

```{r PPC1, fig.cap="Method 1",echo=FALSE}
include_graphics(path = "figure/PPC1.png")
```

```{r PPC2, fig.cap="Method 2",echo=FALSE}
include_graphics(path = "figure/PPC2.png")
```